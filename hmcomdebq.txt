from google.cloud import bigquery

# Construct a BigQuery client object.
client = bigquery.Client()

# Set the name of the dataset and table you want to create.
dataset_id = 'my_dataset'
table_id = 'my_table'

# Create a table reference object.
table_ref = client.dataset(dataset_id).table(table_id)

# Define the schema for your table.
schema = [
    bigquery.SchemaField('column1', 'STRING', mode='REQUIRED'),
    bigquery.SchemaField('column2', 'INTEGER', mode='NULLABLE'),
    bigquery.SchemaField('column3', 'FLOAT', mode='NULLABLE'),
]

# Create the table object.
table = bigquery.Table(table_ref, schema=schema)

# Create the table in BigQuery.
table = client.create_table(table)

print(f'Table {table.table_id} created in dataset {table.dataset_id}.')

loading data

from google.cloud import bigquery
from google.oauth2 import service_account

# Set the name of the dataset and table you want to load data into.
dataset_id = 'my_dataset'
table_id = 'my_table'

# Create a BigQuery client object using your service account credentials.
credentials = service_account.Credentials.from_service_account_file('/path/to/credentials.json')
client = bigquery.Client(credentials=credentials)

# Define the location of your JSON file.
filename = '/path/to/myfile.json'

# Create a table reference object.
table_ref = client.dataset(dataset_id).table(table_id)

# Define the job configuration to load data from a JSON file.
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
)

# Start the job to load data into the table.
with open(filename, 'rb') as source_file:
    job = client.load_table_from_file(
        source_file, table_ref, job_config=job_config
    )

job.result()  # Wait for the job to complete.

print(f'Data loaded into {table_id}.')


from google.cloud import bigquery
from google.oauth2 import service_account
from datetime import datetime

# Set the name of the dataset and table you want to load data into.
dataset_id = 'my_dataset'
table_id = 'my_table'

# Create a BigQuery client object using your service account credentials.
credentials = service_account.Credentials.from_service_account_file('/path/to/credentials.json')
client = bigquery.Client(credentials=credentials)

# Define the location of your JSON file.
filename = '/path/to/myfile.json'

# Create a table reference object.
table_ref = client.dataset(dataset_id).table(table_id)

# Define the job configuration to load data from a JSON file.
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
    schema=[
        bigquery.SchemaField('timestamp', 'TIMESTAMP', mode='REQUIRED'),
        bigquery.SchemaField('column1', 'STRING', mode='REQUIRED'),
        bigquery.SchemaField('column2', 'INTEGER', mode='NULLABLE'),
        bigquery.SchemaField('column3', 'FLOAT', mode='NULLABLE'),
    ],
)

# Start the job to load data into the table.
with open(filename, 'rb') as source_file:
    job = client.load_table_from_file(
        source_file, table_ref, job_config=job_config
    )

job.result()  # Wait for the job to complete.

print(f'Data loaded into {table_id}.')



import json

with open('input.json') as f:
    data = json.load(f)

with open('output.ndjson', 'w') as f:
    for record in data:
        f.write(json.dumps(record) + '\n')
